name: CI/CD

on:
  push:
    branches:
      - "*"
    tags:
      - "*"
  pull_request:

jobs:
  test:
    strategy:
      matrix:
        python-version: ["3.6", "3.7", "3.8", "3.9"]
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@master

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v1
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install python specifics
      run: |
        sudo apt-get update -q
        python -m pip install Cython
        python -m pip install numpy
        python --version
        python -m cython --version
        python -c "import numpy;print(numpy.__version__)"

    - name: Install parsl with no optional extras
      run: |
        make deps

    - name: Run tests
      run: |
        make clean_coverage

    - name: Documentation checks:
      run: |
        pip install .[docs]
        sudo apt-get install -y pandoc
        cd docs

        # check we can build the docs without warnings
        make SPHINXOPTS=-W html

        # check that documentation stubs are up to date, as they are compiled from
        # python code but stored in version control rather than generated as part
        # of doc build.
        rm -rfv stubs
        sphinx-autogen reference.rst userguide/*rst devguide/*rst
        # this will both display any diffs in log output, and fail if there is any diff
        git diff --exit-code

        cd ..

        # assert that none of the runs in this test have put an ERROR message into a
        # database manager log file or monitoring router log file. It would be better if
        # the tests themselves failed immediately when there was a monitoring error, but
        # in the absence of that, this is a dirty way to check.
        bash -c '! grep ERROR runinfo*/*/database_manager.log'
        bash -c '! grep ERROR runinfo*/*/monitoring_router.log'

